5.Factor3 Securing Hosts and container Working environment

Container working environment

- Let's assume that you implemented all of the security controls, such as secure design code, images, and registries, but your application has not come alive yet. It only comes to life and starts running when its image is instantiated as a container. That container, which is nothing but a Linux process, needs an environment and resources to run. If you recall, it needs a container runtime, it needs a host OS, and the capabilities offered by the OS. This environment needs to be secured. You can instantiate a container using a Docker command or in Kubernetes by submitting a part definition to an API server. While doing so, you may override and effectively undo the security configurations you may have put in place at the time of creating images. So let's walk through some security controls to protect the container environment and the host starting with the containers. You know that in your Docker file, you should specify a user to not let the container run as root, but what about when running a container? When starting a container from an image, you have a choice to specify a user ID corresponding to the root, and therefore we're running the container as root. This action overrides any user instructions included previously in the image. Let's talk about another level of privileges that a container can be granted. It's a common misconception that a container running as a root has the highest level of privileges, but that's not true. Recall from prior chapters the list of capabilities that can be added to or drop from a Linux process. Open Container Interface, or OCI specification, defines a set of capabilities that are granted to a container by default. Let's take a quick look at them. If you look carefully in the spec, not all capabilities are turned on as shown by zero. When you start a container with the privileged parameter, you're essentially making all of the system capabilities available to the container process. This is extremely dangerous. Set a container has full access to everything on the host operating system. In other words, by creating a privileged container, you have pierced all boundaries between the container process and the OS kernel, leaving no room for isolation. Granted, there are use cases such as Docker in Docker or system containers that necessitates the use of privileged containers, but most application containers don't need to run in privileged mode.

Container network security

-  Any real business application won't be complete without containers talking to each other. And this communication is enabled by container networking. Now, Docker and Kubernetes take different approaches. In Kubernetes, each pod is assigned an IP address and is expected to communicate with other pods seamlessly regardless of where the pod is running, on the same node or on separate nodes. Kubernetes, however, does not provide the network functionality on its own. Rather, it depends on the container network interface or CNI plugin, which must be set up by an admin separately. So to secure inter-container communication, or to be precise, inter-pod communication in Kubernetes, you need to apply network policies. Through these network policies, you can control what traffic can flow between pods. We will address the network security controls for Kubernetes in the next chapter. But that still leaves us to deal with the security of networks specific to standalone Docker containers. Container platforms such as Docker, provide multiple ways to facilitate communication among containers. These options are implemented via pluggable network drivers. Let's take a quick look at these options before we review the security controls. For a complete description of these drivers, check out the Docker platform documentation. The bridge network driver, which is the default option, allows containers to communicate with each other on the same host. In this diagram, you will be able to spot a Docker bridge virtual interface. A special case of container networking is a closed container. A closed container uses the none network driver. It has only a loopback interface. No incoming or outgoing connections can be made from a closed container. Recall from the previous chapters that an isolated container runs in a separate namespace from that of its host. But there is a special case where you may want the container to run in the same network space as the host. Such a container uses the host network driver. In this option, a container's access to network is identical to that of its host. Underlay network drivers help containers that are on different hosts communicate with each other using the underlying physical interface. Two examples of such drivers are: media access control virtual local area network, or in short macvlan, internet protocol VLAN, or in short ipvlan. These drivers let you treat a container just like a physical host with its own MAC address. This capability comes handy when you're migrating from existing VM-based deployments or have special security or compliance requirements. Overlay network drivers also help containers on different hosts communicate with each other, but by taking a different approach. The overlay network is a virtual network that sits on top of the host specific networks. Overlay networks can be implemented using various virtualization technologies such as virtual extensible LAN or in short vxlan. So to recap, the bridge option allows containers on the same host to talk to each other. Host and none options are used for special container networking. Underlay and overlay networks allow containers across hosts to talk to each other. But by the way, on a side note, Kubernetes can utilize either underlay or overlay network plugins to make the networking possible.


Container port and interface security

- Let's get back to stand alone containers. Even with the bridge network, containers are not directly routable from applications outside the cluster. So how do containers communicate with the outside world? You will need to map a TCP or a UDP port on the host network interface to a target container port. When executing Docker run command, you specify this mapping with -p or -- publish option. For example, this command will forward traffic on port 8088 of the host to port 8080 of the container. In contrast to Kubernetes, Docker does not come with network security policies. So you will need to put best practices and security controls in place to ensure that the communication among containers takes place only between the authorized parties. So let's review some of these controls. The port mapping command that you just saw has a security issue. With this command the container ports are exposed to any IP address which is represented by a wildcard IP address 0.0.0.0 on the host. This is not secure. You should accept connections only on a specific interface. For example, if you run the same command as following, you'll be mapping container port 8080 to a specific interface represented by IP address, 10.2.5.6 and by port 49153. So what happens if you don't specify the port mapping? Well, Docker will automatically map the container port to any available host board in the range of 49153 to 65535. But what if the user does not specify a port between one and 1024? Docker does not restrict that. The problem is that the ports between one and 1024 are deemed privileged and may carry sensitive data. So you should restrict users from mapping to host privileged port. And this can be accomplished using a review of the Docker file in predeployment or by running an audit or scan of the containers after containers are deployed. So let's switch from IP addresses and ports to the processes. Docker Daemon process accepts commands from the clients. The clients use a Docker socket to communicate with the Docker Daemon. Clients can talk to the Daemon over file descriptors, TCP or unique sockets. By default, this unique socket is located at a specific location in the host file system. Access to the socket is protected from other processes on the host. But what do you think will happen if you mount the socket as a volume inside a container? Any process within that container will have full access to send any command to the Daemon. This will result in the container gaining full control of the host, and this is not what you want. So ensure that this socket is not mounted inside the containers. Let's visit the default network bridge one more time. When using the default bridge known as Docker0, by default, all traffic is permitted among containers on the same host. This means every container has read access to the traffic on the container network. This could lead to a breach of confidential information. To avoid this, you have two controls. First disable the default setting in the Docker Daemon configuration. And second, the preferred option is to create a new bridge network and only attach those containers that have a genuine need to talk to each other.

Host OS protection

- Unlike hardware virtualization, in OS virtualization, multiple containers share the same OS. That leads to an expectation of mutual trust between the container and the host OS running that container. So far we have been discussing container and application security. But the host is equally accountable in the security of this ecosystem. In fact, the impact of a host being breached is way more amplified than that of a container breach. So let's walk through security controls to protect the host. Start with minimizing the host's attack surface. A typical Linux operating system has way more services running than you need to support container applications. Here you have two options to choose from. First, run a minimal OS that has been custom built for running containers, for example, VMware Photon or Red Hat CoreOS. These so called thin OSs have been stripped off of many components and services not typically required to run containers. Your second option is to keep using traditional operating system as mandated by your organization, but apply special hardening techniques to reduce the attack surface. You have fewer resources to rely on, the CIS Benchmarks and the NIST guide to server security. Look at the workloads you're planning to run on your hosts, are you mixing both containerized and non-containerized workloads on the same VM? Move non-containerized workloads such as general services and applications to separate hosts. Look, if you don't do that, it's likely your general workloads are not going to be managed by an automated orchestrator such as Kubernetes. That means your admin and operations team will need to manually access the host to start, stop and manage your services. And that will have two consequences. First, there is a potential for error and fraud. Obviously, you will need to grant permissions to users and operations to log in to host maintain the applications. This opens up your host to new attack vectors, either internal or external. And second, admins making changes to a live host will lead to a drift in the host configuration from its baseline. And that will result in a disparity in the configuration of hosts running in your cluster. Just like containers, you should make your hosts immutable as well. And lastly, if your container does get breached, you will limit the spillover to only applications running on that host. By agreeing to run only containerized workloads, you have already minimized the need for a human to be in direct contact with the hosts. Let Kubernetes run and manage your containers. With that said, there are still operational requirements that traditional hosts will have, for example, patching. Just as you do with your containers, treat your hosts as cattle, not pets. Instead of patching, take the host VM out of service, reimage and put it back into the pools of nodes under Kubernetes. Now, while your host is down, trust your Kubernetes orchestration 


